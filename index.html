<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anwoy Chatterjee</title>

    <meta name="author" content="Anwoy Chatterjee">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/logo.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Anwoy Chatterjee
                </p>
                <p>I am a third-year PhD student at <a href="https://home.iitd.ac.in/">IIT Delhi</a>, advised by <a href="https://tanmoychak.com/">Prof. Tanmoy Chakraborty</a>. 
					My research focuses on making Large Language Models (LLMs) more robust, reliable, and interpretable, part of a broader mission to build safe and transparent AI systems.                
				</p>
                <p> 
					I am grateful to be supported by the <a href="https://research.google/programs-and-events/phd-fellowship/recipients/">Google PhD Fellowship</a>.
					I also collaborate closely with <a href="http://sumitbhatia.net/">Dr. Sumit Bhatia</a> and have previously interned (twice) at the Media and Data Science Research (MDSR) Lab at Adobe Inc.                
				</p>

                <p>
                  Before my PhD, I earned my Bachelor's in Computer Science and Engineering from <a href="https://iitbhu.ac.in/">IIT(BHU), Varanasi</a> where I worked on Computer Vision and Graph Neural Networks
                  with <a href="https://www.iitbhu.ac.in/dept/cse/people/rajeevcse">Prof. Rajeev Srivastava</a> and <a href="https://www.iitbhu.ac.in/dept/cse/people/pratikcse">Prof. Pratik Chattopadhyay</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:anwoychatterjee@gmail.com">Email</a> &nbsp;|&nbsp;
                  <a href="data/Anwoy_CV.pdf">CV</a> &nbsp;|&nbsp;
                  <!-- <a href="data/anwoy-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://dblp.org/pid/377/6292">DBLP</a> &nbsp;|&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=-WDvDV8AAAAJ">Google Scholar</a> &nbsp;|&nbsp;
                  <a href="https://www.linkedin.com/in/anwoy-chatterjee/">LinkedIn</a> &nbsp;|&nbsp;
                  <a href="https://x.com/anwoy_">X</a> &nbsp;|&nbsp;
                  <a href="https://bsky.app/profile/anwoy.bsky.social">Bluesky</a> &nbsp;|&nbsp;
                  <a href="https://github.com/C-anwoy/">GitHub</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/anwoy.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/anwoy.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Recent Updates</h2>
                <ul style="list-style-type: disc; padding-left: 20px;">
					<li><strong>August 2025:</strong> Our <a href="https://aclanthology.org/2025.emnlp-main.756/">paper</a> on cultural evaluation of multilingual language models got accepted to EMNLP 2025. This was a nice collaboration with the <a href="http://nlp.cs.ucsb.edu/index.html">UCSB NLP Group</a>.</li>
					<li><strong>July 2025:</strong> Our <a href="https://direct.mit.edu/tacl/article/doi/10.1162/TACL.a.42/133798/On-the-Effect-of-Instruction-Tuning-Loss-on">paper</a> on the effect of instruction tuning loss on the generalization capability of language models got accepted to the Transactions of the Association for Computational Linguistics (TACL). This work is yet another outcome of my internship at Adobe!</li>
					<li><strong>January 2025:</strong> Started my (second) internship at Adobe's Media and Data Science Research (MDSR) Lab.</li>
                  	<li><strong>November 2024:</strong> Delivered an invited talk at Google Deepmind, Bangalore.</li>
                 	<li><strong>November 2024:</strong> Presented our work at Amazon Research Days 2024.</li>
                  	<li><strong>September 2024:</strong> Our <a href="https://aclanthology.org/2024.findings-emnlp.852/">paper</a> on a novel prompt sensitivity index got accepted to EMNLP (Findings) 2024. This work 
                    was done during my internship at Adobe.</li>
                  	<li><strong>August 2024:</strong> Awarded <a href="https://research.google/programs-and-events/phd-fellowship/recipients/">Google PhD Fellowship 2024 in NLP</a>.</li>
                  	<li><strong>May 2024:</strong> Our <a href="https://aclanthology.org/2024.acl-long.621/">paper</a> on cross-task in-context learning got accepted to ACL 2024.</li>
                  	<!--li><strong>January 2024:</strong> Attended <a href="https://sites.google.com/view/researchweek24/home">Google Research Week 2024</a>.</li-->
                </ul>
              </td>
            </tr>
          </tbody></table>

		<!-- Education Section -->
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  <tr>
		    <td style="padding:16px;width:100%;vertical-align:middle">
		      <h2>Education</h2>
		      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		        <tr>
		          <td style="width:20%;vertical-align:middle;text-align:center;">
		            <img src="images/iit_delhi_logo.png" alt="IIT Delhi Logo" style="width:50px;height:auto;">
		          </td>
		          <td style="padding:8px;width:80%;vertical-align:middle;">
		            <strong>Doctor of Philosophy in Artificial Intelligence</strong><br>
		            Indian Institute of Technology, Delhi<br>
		            <span style="color:gray;">2022 – Present</span>
		          </td>
		        </tr>
		        <tr>
		          <td style="width:20%;vertical-align:middle;text-align:center;">
		            <img src="images/iit_bhu_logo.png" alt="IIT BHU Logo" style="width:50px;height:auto;">
		          </td>
		          <td style="padding:8px;width:80%;vertical-align:middle;">
		            <strong>Bachelor of Technology in Computer Science and Engineering</strong><br>
		            Indian Institute of Technology (BHU), Varanasi<br>
		            <span style="color:gray;">2018 – 2022</span>
		          </td>
		        </tr>
		      </table>
		    </td>
		  </tr>
		</tbody></table>

		
		
		<!-- Experience Section -->
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  <tr>
		    <td style="padding:16px;width:100%;vertical-align:middle">
		      <h2>Experience</h2>
		      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tr>
		          <td style="width:20%;vertical-align:middle;text-align:center;">
		            <img src="images/adobe_logo.png" alt="Adobe Logo" style="width:80px;height:auto;">
		          </td>
		          <td style="padding:8px;width:80%;vertical-align:middle;">
		            <strong>Research Intern</strong><br>
		            Media and Data Science Research (MDSR) Lab, Adobe Inc.<br>
		            <strong>Mentor: </strong><a href="http://sumitbhatia.net/">Dr. Sumit Bhatia</a><br>
		            <span style="color:gray;">January 2025 – July 2025</span>
		          </td>
		        </tr>
			      
		        <tr>
		          <td style="width:20%;vertical-align:middle;text-align:center;">
		            <img src="images/adobe_logo.png" alt="Adobe Logo" style="width:80px;height:auto;">
		          </td>
		          <td style="padding:8px;width:80%;vertical-align:middle;">
		            <strong>Research Intern</strong><br>
		            Media and Data Science Research (MDSR) Lab, Adobe Inc.<br>
		            <strong>Mentor: </strong><a href="http://sumitbhatia.net/">Dr. Sumit Bhatia</a><br>
		            <span style="color:gray;">May 2024 – August 2024</span>
		          </td>
		        </tr>
		      </table>
		    </td>
		  </tr>
		</tbody></table>


          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in natural language processing, deep learning, generative AI, and model interpretability. My current research revolves 
                  around understanding the workings of large language models, making them more robust and reliable, and enabling their effective in-context adaptation 
                  in low-resource settings.<!--span class="highlight">highlighted</span-->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <!-- Publications Section -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:16px;width:100%;vertical-align:middle">
      <h3>Publications</h3>
      <table style="width:100%;border:0px;border-spacing:0px;margin-right:auto;margin-left:auto;">

		  <!-- Publication 5: HIDE -->
        <tr>
          <!-- td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images/pubs/hide.png" alt="hide-image" style="width:160px; height:auto;">
		</td-->
          <td style="padding:8px;width:100%;vertical-align:middle">
            <span class="papertitle" style="color:rgb(9, 114, 220);">HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations</span>
            <br>
            <strong>Anwoy Chatterjee</strong>, Yash Goel, Tanmoy Chakraborty
            <br>
            <em>Preprint (Under Review)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2506.17748">paper</a> &nbsp;/&nbsp;
            <a href="https://github.com/C-anwoy/HIDE">code</a> &nbsp;/&nbsp;
            <a href="data/ChatterjeeHIDE.bib">bibtex</a>
            <!-- p>In this work, we propose a single-pass, training-free approach for Hallucination detectIon via Decoupled rEpresentations (HIDE).</p-->
          </td>
        </tr>
		  
		  <!-- Publication 4: XNationQA -->
        <tr>
          <!-- td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images/pubs/emnlp25.png" alt="emnlp25-image" style="width:160px; height:auto;">
		</td -->
          <td style="padding:8px;width:100%;vertical-align:middle">
            <span class="papertitle" style="color:rgb(9, 114, 220);">Do You Know About My Nation? Investigating Multilingual Language Models’ Cultural Literacy Through Factual Knowledge</span>
            <br>
            Eshaan Tanwar, <strong>Anwoy Chatterjee</strong>, Michael Saxon, Alon Albalak, William Yang Wang, Tanmoy Chakraborty
            <br>
            <em>EMNLP</em>, 2025
            <br>
            <a href="https://aclanthology.org/2025.emnlp-main.756/">paper</a> &nbsp;/&nbsp;
            <a href="https://github.com/EshaanT/XNationQA">code</a> &nbsp;/&nbsp;
            <a href="data/TanwarEMNLP2025.bib">bibtex</a>
            <!--p>In this work, we introduce XNationQA for investigating the cultural literacy of multilingual LLMs. Interestingly, we observe that even multilingual models demonstrate greater knowledge of cultural information in English than in the dominant language of the respective culture.
		  </p-->
          </td>
        </tr>
		  
		  <!-- Publication 3: WIT -->
        <tr>
          <!--td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images/pubs/tacl25.png" alt="tacl25-image" style="width:160px; height:auto;">
		</td-->
          <td style="padding:8px;width:100%;vertical-align:middle">
            <span class="papertitle" style="color:rgb(9, 114, 220);">On the Effect of Instruction Tuning Loss on Generalization</span>
            <br>
            <strong>Anwoy Chatterjee*</strong>, H S V N S Kowndinya Renduchintala*, Sumit Bhatia, Tanmoy Chakraborty
            <br>
            <em>Transactions of the Association for Computational Linguistics (TACL)</em>, 2025
            <br>
            <a href="https://direct.mit.edu/tacl/article/doi/10.1162/TACL.a.42/133798/On-the-Effect-of-Instruction-Tuning-Loss-on">paper</a> &nbsp;/&nbsp;
            <a href="https://github.com/kowndinya-renduchintala/WIT">code</a> &nbsp;/&nbsp;
            <a href="data/ChatterjeeTACL2025.bib">bibtex</a>
            <!--p>In this work, we systematically investigate the impact of differentially weighting prompt and response tokens in instruction tuning loss, show that the conventional instruction tuning loss is sub-optimal, and propose Weighted Instruction Tuning (WIT) as a better alternative to conventional instruction tuning.</p-->
          </td>
        </tr>
		  
        <!-- Publication 2: POSIX -->
        <tr>
          <!--td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images/pubs/emnlp24.png" alt="emnlp24-image" style="width:160px; height:auto;">
		</td-->
          <td style="padding:8px;width:100%;vertical-align:middle">
            <span class="papertitle" style="color:rgb(9, 114, 220);">POSIX: A Prompt Sensitivity Index For Large Language Models</span>
            <br>
            <strong>Anwoy Chatterjee*</strong>, H S V N S Kowndinya Renduchintala*, Sumit Bhatia, Tanmoy Chakraborty
            <br>
            <em>EMNLP (Findings)</em>, 2024
            <br>
            <a href="https://aclanthology.org/2024.findings-emnlp.852/">paper</a> &nbsp;/&nbsp;
            <a href="https://github.com/kowndinya-renduchintala/POSIX">code</a> &nbsp;/&nbsp;
            <a href="https://youtu.be/WV1hfWmIAIM">video</a> &nbsp;/&nbsp;
            <a href="data/ChatterjeeEMNLP2024.bib">bibtex</a>
            <!--p>We propose POSIX – a novel PrOmpt Sensitivity IndeX as a reliable measure of prompt sensitivity, offering a more comprehensive evaluation of LLM performance.</p-->
          </td>
        </tr>
		  
        <!-- Publication 1: CTICL -->
        <tr>
          <!--td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images/pubs/acl24.jpg" alt="acl24-image" style="width:160px; height:auto;">
		</td-->
          <td style="padding:8px;width:100%;vertical-align:middle">
            <span class="papertitle" style="color:rgb(9, 114, 220);">Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks</span>
            <br>
            <strong>Anwoy Chatterjee*</strong>, Eshaan Tanwar*, Subhabrata Dutta, Tanmoy Chakraborty
            <br>
            <em>ACL</em>, 2024
            <br>
            <a href="https://aclanthology.org/2024.acl-long.621/">paper</a> &nbsp;/&nbsp;
            <a href="https://github.com/C-anwoy/Cross-Task-ICL">code</a> &nbsp;/&nbsp;
            <a href="https://youtu.be/EdsLL-55z5w">video</a> &nbsp;/&nbsp;
            <a href="data/ChatterjeeACL2024.bib">bibtex</a>
            <!--p>In this work, we offer a first-of-its-kind exploration of LLMs’ ability to solve novel tasks based on contextual signals from different task examples. </p-->
          </td>
        </tr>
      </table>
    </td>
  </tr>
</tbody></table>



          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <!--tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h4>Micropapers</h4>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr-->


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h4>Recorded <br>Lectures/<br>Tutorials/<br>Talks</h4>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
    
								<strong><a href="https://youtu.be/DfpE1RP_xbs">Panel discussion</a></strong> on the current state of LLM research in academia and industry.
								<br>
								<strong><a href="https://youtu.be/IJsigZ7tM94">Lecture on "Interpretability: Demystifying the Black-Box LMs"</a></strong> as part of ELL881/AIL821 at IIT Delhi.
                <br>
								<strong><a href="https://youtu.be/WV1hfWmIAIM">Virtual presentation</a></strong> of our EMNLP'24 paper - "POSIX: A Prompt Sensitivity Index For Large Language Models".
                <br>
								<strong><a href="https://youtu.be/EdsLL-55z5w">Virtual presentation</a></strong> of our ACL'24 paper - "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks".
              </td>
            </tr>

            <!-- tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h4>Academic Service</h4>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr -->
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h4><br>Teaching <br> <br></h4>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <strong>Graduate Teaching Assistant</strong>, <a href="https://lcs2.in/llm2501">ELL8299/ELL881/AIL861 (Advanced Large Language Models), Fall 2025</a>
                <br>
				<strong>Teaching Assistant</strong>, Introduction to Large Language Models, NPTEL (<a href="https://onlinecourses.nptel.ac.in/noc25_cs45/preview">January 2025</a>, <a href="https://onlinecourses.nptel.ac.in/noc25_cs161/preview">July 2025</a>)
                <br>
                <strong>Graduate Teaching Assistant</strong>, <a href="https://lcs2-iitd.github.io/ELL884-2402/">ELL884 (Deep Learning for NLP), Spring 2025</a>
                <br>
                <strong>Graduate Teaching Assistant</strong>, <a href="https://lcs2-iitd.github.io/ELL881-AIL821-2401/">ELL881/AIL821 (Large Language Models: Introduction and Recent Advances), Fall 2024</a>
                <br>
                <strong>Graduate Teaching Assistant</strong>, <a href="https://sites.google.com/view/ell880-iitd-2023">ELL880 (Social Network Analysis), Fall 2023</a>
                
              </td>
            </tr>
            
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h4>Talks/<br>Posters</h4>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <strong>Invited talk</strong> at Google Deepmind, Bangalore, India, Nov 22, 2024.
                <br>
                <strong>Poster presentation</strong> at Amazon Research Days 2024, Bangalore, India.
                <br>
                <strong>Poster presentation</strong> at ACL 2024, Bangkok, Thailand.

              </td>
            </tr>
          </tbody></table>

          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
